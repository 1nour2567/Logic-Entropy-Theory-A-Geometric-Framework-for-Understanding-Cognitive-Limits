"""
GTMLM Mamba Ultimate Edition - Complete Implementation
æ•´åˆï¼šåèº«æ€§è‡ªé€‚åº”ç½‘ç»œ + é€»è¾‘çš„ç†µ + å½¢å¼è®¤è¯†è®º + è®¤çŸ¥çº¤ç»´ä¸›ç†è®º
Author: åŸºäºæ‚¨çš„ç†è®ºä½“ç³»
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import math
import numpy as np
from torch.utils.data import Dataset, DataLoader
import random
from collections import OrderedDict

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ====================== æ ¸å¿ƒå¸¸é‡ ======================
N_CLASSES = 5  # 5-wayåˆ†ç±»
AXIOM_VOCAB_START = 800
AXIOM_CLUSTERS = [
    [800, 801, 802],  # ç±»åˆ«0çš„å…¬ç†
    [810, 811, 812],  # ç±»åˆ«1çš„å…¬ç†
    [820, 821, 822],  # ç±»åˆ«2çš„å…¬ç†
    [830, 831, 832],  # ç±»åˆ«3çš„å…¬ç†
    [840, 841, 842]   # ç±»åˆ«4çš„å…¬ç†
]
MAX_AXIOMS = 100  # å…¬ç†è®°å¿†æ± å®¹é‡

# ====================== 1. è®¤çŸ¥çº¤ç»´ä¸›æ ¸å¿ƒç»„ä»¶ ======================
class CognitiveFiberBundle(nn.Module):
    """
    è®¤çŸ¥çº¤ç»´ä¸› - å®šä¹‰1.1ï¼ˆå½¢å¼è®¤è¯†è®ºï¼‰
    å®ç°å®šç†1ï¼šä»»ä½•ç†æ€§ç³»ç»Ÿçš„è®¤çŸ¥ç»“æ„éƒ½å¯ä»¥è¡¨ç¤ºä¸ºè®¤çŸ¥çº¤ç»´ä¸›çš„æˆªé¢
    """
    def __init__(self, base_dim=256, fiber_dim=128, connection_dim=64):
        super().__init__()
        
        # åº•æµå½¢ï¼šè®¤çŸ¥çŠ¶æ€ç©ºé—´
        self.base_dim = base_dim  # dim(M) = NÂ²
        
        # çº¤ç»´ï¼šå¯èƒ½çš„è®¤çŸ¥å»ºæ„
        self.fiber_dim = fiber_dim
        
        # ç»“æ„ç¾¤ï¼šè®¤çŸ¥å¯¹ç§°æ€§ï¼ˆAut(V) Ã— â„^Kï¼‰
        self.structure_group = nn.ParameterDict({
            'linear': nn.Parameter(torch.eye(fiber_dim)),  # GL(fiber_dim)éƒ¨åˆ†
            'constraint': nn.Parameter(torch.randn(10, fiber_dim) * 0.01)  # â„^Kéƒ¨åˆ†
        })
        
        # è¿æ¥å½¢å¼ï¼šç¼–ç æ¨ç†è§„åˆ™ï¼ˆå®šä¹‰5.1ï¼‰
        self.connection = nn.Parameter(
            torch.eye(fiber_dim) + torch.randn(fiber_dim, fiber_dim) * 1e-3
        )
        
        # æ›²ç‡å½¢å¼ï¼šé€»è¾‘ä¸ä¸€è‡´æ€§ï¼ˆå®šç†5.3ï¼‰
        self.curvature = None  # åŠ¨æ€è®¡ç®—
        
        # è®¤çŸ¥è”ç»œï¼ˆé™ˆ-è¥¿è’™æ–¯ç†è®ºï¼‰
        self.chern_simons_weight = nn.Parameter(torch.tensor(1.0))
        
    def parallel_transport(self, state, path_length=5):
        """
        å®šç†5.2ï¼šè¯æ˜å³å¹³è¡Œç§»åŠ¨
        æ²¿ç€è®¤çŸ¥è·¯å¾„å¹³è¡Œç§»åŠ¨çŠ¶æ€
        """
        transported = state
        for _ in range(path_length):
            # è¿æ¥ä½œç”¨ï¼šâˆ‡_v s
            transported = torch.matmul(transported, self.connection)
            transported = F.normalize(transported, dim=-1)
        return transported
    
    def compute_curvature(self, state):
        """
        è®¡ç®—æ›²ç‡ï¼šF = dA + A âˆ§ A
        å¯¹åº”å®šç†5.3ï¼šä¸å¯åˆ¤å®šå¯†åº¦å³æ›²ç‡
        """
        # ç®€åŒ–çš„æ›²ç‡è®¡ç®—
        A = self.connection
        F = torch.matmul(A, A) - torch.matmul(A.t(), A.t()).t()
        self.curvature = torch.norm(F) / (self.fiber_dim ** 2)
        return self.curvature
    
    def compute_chern_simons(self):
        """
        å®šç†5.4ï¼šæ€»é€»è¾‘ç†µå³é™ˆ-è¥¿è’™æ–¯ä¸å˜é‡
        H(L) = k/4Ï€ âˆ« (A âˆ§ dA + 2/3 A âˆ§ A âˆ§ A) mod â„¤
        """
        if self.curvature is None:
            self.compute_curvature(torch.randn(1, self.fiber_dim))
        
        # ç®€åŒ–çš„é™ˆ-è¥¿è’™æ–¯ä¸å˜é‡è®¡ç®—
        A = self.connection
        dA = A - A.t()  # ç®€åŒ–çš„å¤–å¾®åˆ†
        
        # A âˆ§ dA
        term1 = torch.trace(torch.matmul(A, dA))
        
        # A âˆ§ A âˆ§ A
        A_cubed = torch.matmul(torch.matmul(A, A), A)
        term2 = (2/3) * torch.trace(A_cubed)
        
        cs_invariant = (self.chern_simons_weight / (4 * math.pi)) * (term1 + term2)
        
        # æ¨¡æ•´æ•°æ‹“æ‰‘ä¸å˜é‡
        return cs_invariant - torch.floor(cs_invariant)
    
    def holonomy(self, state, loop_steps=8):
        """
        è®¡ç®—holonomyï¼šæ²¿é—­åˆç¯è·¯å¹³è¡Œç§»åŠ¨çš„å·®å¼‚
        å¯¹åº”é€»è¾‘çš„ä¸å®Œå¤‡æ€§
        """
        start_state = state
        transported = self.parallel_transport(state, loop_steps)
        
        # è¿”å›èµ·ç‚¹ï¼Œè®¡ç®—å·®å¼‚
        hol = torch.norm(transported - start_state, dim=-1).mean()
        return hol

# ====================== 2. åèº«æ€§è‡ªé€‚åº”ç½‘ç»œ ======================
class ReflexiveAdaptiveNetwork(nn.Module):
    """
    åèº«æ€§è‡ªé€‚åº”ç½‘ç»œ - å®šä¹‰1.1.1
    äº”å…ƒç»„ ğ’©, â„°, ğ’¢, ğ’Ÿ, â„› çš„å®ç°
    """
    def __init__(self, node_dim=256, constraint_dim=64):
        super().__init__()
        
        # èŠ‚ç‚¹é›†åˆ ğ’©ï¼šè®¤çŸ¥å…ƒ
        self.node_dim = node_dim
        self.nodes = nn.Parameter(torch.randn(MAX_AXIOMS, node_dim) * 0.01)
        
        # è¾¹é›†åˆ â„°ï¼šç›¸äº’ä½œç”¨
        self.adjacency = nn.Parameter(
            torch.eye(MAX_AXIOMS) * 0.9 + torch.randn(MAX_AXIOMS, MAX_AXIOMS) * 0.1
        )
        
        # è‡ªæŒ‡çº¦æŸé›†åˆ ğ’¢
        self.constraints = nn.Parameter(torch.randn(constraint_dim, node_dim) * 0.01)
        
        # åŠ¨åŠ›å­¦æ¼”åŒ–è§„åˆ™ ğ’Ÿï¼ˆå®šç†2.1.1ï¼‰
        self.evolution_weight = nn.Parameter(torch.eye(node_dim))
        self.noise_scale = nn.Parameter(torch.tensor(0.01))
        
        # åèº«æ€§æ“ä½œè§„åˆ™ â„›
        self.reflexive_load = nn.Parameter(torch.tensor(0.5))  # åèº«æ€§è´Ÿè·Î»
        
        # è‡ªé€‚åº”è€¦åˆï¼ˆå®šç†2.2.1ï¼‰
        self.hebbian_eta = nn.Parameter(torch.tensor(0.1))  # å­¦ä¹ ç‡Î·
        self.decay_alpha = nn.Parameter(torch.tensor(0.01))  # è¡°å‡ç³»æ•°Î±
        self.reflexive_beta = nn.Parameter(torch.tensor(0.1))  # åèº«æ€§è°ƒèŠ‚å¼ºåº¦Î²
        
    def evolve_nodes(self, node_states, steps=1):
        """
        å®šç†2.1.1ï¼šèŠ‚ç‚¹çŠ¶æ€æ¼”åŒ–
        ds_i = F_i(s, J, g)dt + Ïƒ_i dW_t^i
        """
        results = []
        current = node_states
        
        for step in range(steps):
            # ç¡®å®šæ€§æ¼”åŒ– F_i
            deterministic = torch.matmul(current, self.evolution_weight)
            
            # èµ«å¸ƒå­¦ä¹ ï¼šdJ_ij/dt = Î·Â·[s_i s_j - Î±J_ij + Î²g_kÂ·Ï†(J_ij)]
            interactions = torch.matmul(current, self.adjacency)
            hebbian = torch.matmul(current.t(), current)
            constraint_effect = torch.matmul(self.constraints, current.t())
            
            # æ›´æ–°è¿æ¥ï¼ˆç®€åŒ–ï¼‰
            J_update = self.hebbian_eta * (
                hebbian - 
                self.decay_alpha * self.adjacency +
                self.reflexive_beta * constraint_effect.t()
            )
            
            # æ·»åŠ å™ªå£°ï¼ˆç»´çº³è¿‡ç¨‹ï¼‰
            noise = torch.randn_like(current) * self.noise_scale
            
            # æ›´æ–°çŠ¶æ€
            new_state = deterministic + interactions + noise
            current = F.normalize(new_state, dim=-1)
            results.append(current)
        
        return results[-1] if results else node_states
    
    def reflexive_operation(self, node_states, reflexivity_level=None):
        """
        åèº«æ€§æ“ä½œ â„›ï¼šç³»ç»Ÿå¯¹è‡ªèº«çš„è®¤çŸ¥å’Œæ“ä½œ
        å¯¹åº”å®šç†3.1.2ï¼šåèº«æ€§å¥‡ç‚¹çš„å­˜åœ¨æ€§
        """
        if reflexivity_level is None:
            reflexivity_level = torch.sigmoid(self.reflexive_load)
        
        # è‡ªæŒ‡æ“ä½œï¼šèŠ‚ç‚¹æŸ¥è¯¢è‡ªèº«è¡¨ç¤º
        self_attention = F.softmax(
            torch.matmul(node_states, node_states.t()) / 
            math.sqrt(self.node_dim), 
            dim=-1
        )
        
        # åº”ç”¨è‡ªæŒ‡çº¦æŸ
        constrained = torch.matmul(self_attention, node_states)
        
        # åèº«æ€§è´Ÿè·å½±å“
        reflexive_output = (
            (1 - reflexivity_level) * node_states + 
            reflexivity_level * torch.matmul(constrained, self.constraints.t())
        )
        
        # æ£€æµ‹åèº«æ€§å¥‡ç‚¹ï¼ˆdet(âˆ‚â„›/âˆ‚Î») = 0ï¼‰
        jacobian_det = torch.det(self_attention + 1e-6 * torch.eye(self_attention.size(0)))
        singularity_indicator = torch.abs(jacobian_det)
        
        return reflexive_output, singularity_indicator

# ====================== 3. Mambaå¢å¼ºçŠ¶æ€ç©ºé—´æ¨¡å‹ ======================
class EnhancedMambaBlock(nn.Module):
    """
    MambaBlockçš„å¢å¼ºç‰ˆï¼Œé›†æˆå‡ ä½•çº¦æŸ
    æ”¯æŒé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´å’Œçº¤ç»´ä¸›ç»“æ„
    """
    def __init__(self, d_model=256, d_state=16, d_conv=4, expand=2, 
                 fiber_bundle=None, reflexive_network=None):
        super().__init__()
        
        self.d_model = d_model
        self.d_state = d_state
        self.d_inner = expand * d_model
        self.d_conv = d_conv
        
        # æ ‡å‡†Mambaç»„ä»¶
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)
        self.conv1d = nn.Conv1d(
            self.d_inner, self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv-1
        )
        self.x_proj = nn.Linear(self.d_inner, self.d_inner + 2 * d_state, bias=False)
        self.dt_proj = nn.Linear(self.d_inner, self.d_inner, bias=True)
        
        # çŠ¶æ€çŸ©é˜µAï¼ˆè´Ÿå¯¹æ•°åˆå§‹åŒ–ï¼‰
        A = torch.arange(1, d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)
        self.A_log = nn.Parameter(torch.log(A))
        
        # Då‚æ•°
        self.D = nn.Parameter(torch.ones(self.d_inner))
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)
        
        # å‡ ä½•å¢å¼ºï¼šçº¤ç»´ä¸›é›†æˆ
        self.fiber_bundle = fiber_bundle
        if fiber_bundle is None:
            self.fiber_bundle = CognitiveFiberBundle(base_dim=d_model, fiber_dim=d_model//2)
        
        # åèº«æ€§å¢å¼º
        self.reflexive_network = reflexive_network
        if reflexive_network is None:
            self.reflexive_network = ReflexiveAdaptiveNetwork(node_dim=d_model)
        
        # é€»è¾‘ç†µç›‘æµ‹
        self.logical_entropy = nn.Parameter(torch.tensor(0.0))
        
    def forward(self, x, reflexive_load=0.5):
        B, L, D = x.shape
        
        # 1. è¾“å…¥æŠ•å½±
        xz = self.in_proj(x)
        x_in = xz[..., :self.d_inner]
        x_conv, z = xz.chunk(2, dim=-1)
        
        # 2. å·ç§¯å¤„ç†
        x_conv = x_conv.transpose(1, 2)
        x_conv = self.conv1d(x_conv)[..., :L]
        x_conv = x_conv.transpose(1, 2)
        x_conv = F.silu(x_conv)
        
        # 3. è®¡ç®—SSMå‚æ•°
        x_proj_out = self.x_proj(x_conv)
        dt, Bc, C = torch.split(
            x_proj_out,
            [self.d_inner, self.d_state, self.d_state],
            dim=-1
        )
        dt = F.softplus(self.dt_proj(dt))
        
        # 4. ç¦»æ•£åŒ–
        A = -torch.exp(self.A_log)
        dA = torch.exp(dt.unsqueeze(-1) * A)
        dB = dt.unsqueeze(-1) * Bc.unsqueeze(2)
        
        # 5. é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹
        h = torch.zeros(B, self.d_inner, self.d_state, device=x.device)
        ys = []
        
        for i in range(L):
            # åœ¨æ¯ä¸€æ­¥åŠ å…¥å‡ ä½•çº¦æŸ
            if i % 4 == 0 and self.fiber_bundle is not None:
                # å¹³è¡Œç§»åŠ¨å½“å‰çŠ¶æ€
                fiber_state = x_conv[:, i:i+1, :self.fiber_bundle.fiber_dim]
                transported = self.fiber_bundle.parallel_transport(fiber_state)
                x_conv[:, i:i+1, :self.fiber_bundle.fiber_dim] = transported
            
            h = h * dA[:, i] + dB[:, i] * x_conv[:, i:i+1, :]
            y = (h * C[:, i:i+1, :]).sum(-1)
            ys.append(y)
        
        y = torch.stack(ys, dim=1)
        
        # 6. Dé¡¹å’Œé—¨æ§
        y = y + x_conv * self.D.unsqueeze(0).unsqueeze(0)
        y = y * F.silu(z)
        
        # 7. è¾“å‡ºæŠ•å½±
        y = self.out_proj(y)
        
        # 8. æ®‹å·®è¿æ¥
        output = y + x_in
        
        # 9. åèº«æ€§å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if reflexive_load > 0 and self.reflexive_network is not None:
            # é‡å¡‘ä¸ºèŠ‚ç‚¹å½¢å¼
            node_states = output.mean(1)  # [B, D]
            reflexive_output, singularity = self.reflexive_network.reflexive_operation(
                node_states, reflexive_load
            )
            output = output + reflexive_output.unsqueeze(1)
            
            # æ›´æ–°é€»è¾‘ç†µï¼ˆå®šç†4.2.4ï¼‰
            curvature = self.fiber_bundle.compute_curvature(node_states)
            self.logical_entropy.data = 0.9 * self.logical_entropy + 0.1 * curvature
        
        return output

# ====================== 4. é€»è¾‘ç†µç›‘æ§å™¨ ======================
class LogicalEntropyMonitor(nn.Module):
    """
    é€»è¾‘ç†µç›‘æ§å™¨ - åŸºäºã€Šé€»è¾‘çš„ç†µã€‹ç†è®º
    å®šç†4.1ï¼šé€»è¾‘éå…¨èƒ½å®šç†
    å®šç†4.2.4ï¼šä¸´ç•ŒæŒ‡æ•°ä¸æ ‡åº¦å¾‹
    """
    def __init__(self, system_complexity=1000):
        super().__init__()
        
        # ä¸å¯åˆ¤å®šæ€§å¯†åº¦ D_e(L)ï¼ˆå®šä¹‰2.3ï¼‰
        self.undecidable_density = nn.Parameter(torch.tensor(0.01))
        
        # è§„åˆ™å¤æ‚æ€§æº¢ä»· Î£Î”Cï¼ˆå®šä¹‰2.3ï¼‰
        self.complexity_premium = nn.Parameter(torch.tensor(0.0))
        
        # ä¸´ç•Œå‚æ•°
        self.critical_load = nn.Parameter(torch.tensor(0.7))  # Î»_c
        self.critical_exponents = nn.ParameterDict({
            'nu': nn.Parameter(torch.tensor(1.0)),  # å…³è”é•¿åº¦æŒ‡æ•°
            'beta': nn.Parameter(torch.tensor(0.5)),  # åºå‚é‡æŒ‡æ•°
            'gamma': nn.Parameter(torch.tensor(1.0)),  # å“åº”å‡½æ•°æŒ‡æ•°
        })
        
        # é€»è¾‘çƒ­åŠ›å­¦é‡ï¼ˆå®šä¹‰4.3.1ï¼‰
        self.logical_temperature = nn.Parameter(torch.tensor(1.0))
        self.logical_internal_energy = nn.Parameter(torch.tensor(0.0))
        
        # Chaitin Î©æ•°ä¼°è®¡
        self.omega_estimate = nn.Parameter(torch.tensor(0.5))
        
    def compute_entropy(self, reflexive_load, system_complexity):
        """
        è®¡ç®—é€»è¾‘ç†µ H(L) = f(D_e, Î£Î”C)
        å®šç†5.1ï¼šÎ©-ç†µç­‰ä»·å®šç†
        """
        # åŸºç¡€ç†µé¡¹
        base_entropy = torch.log(1 + self.undecidable_density)
        
        # å¤æ‚æ€§æº¢ä»·é¡¹
        premium_term = self.complexity_premium * system_complexity / 1000
        
        # åèº«æ€§è´Ÿè·é¡¹ï¼ˆä¸´ç•Œè¡Œä¸ºï¼‰
        load_diff = reflexive_load - self.critical_load
        if load_diff > 0:
            # è¶…ä¸´ç•Œï¼šH ~ (Î» - Î»_c)^Î²
            critical_term = torch.pow(load_diff, self.critical_exponents['beta'])
        else:
            # äºšä¸´ç•Œï¼šæŒ‡æ•°å¢é•¿
            critical_term = torch.exp(-torch.abs(load_diff) * 10)
        
        # Chaitin Î©é¡¹
        omega_term = self.critical_exponents['gamma'] * self.omega_estimate
        
        # æ€»é€»è¾‘ç†µ
        total_entropy = base_entropy + premium_term + critical_term + omega_term
        
        return total_entropy
    
    def update_critical_behavior(self, reflexive_load, entropy_gradient):
        """
        æ›´æ–°ä¸´ç•Œè¡Œä¸ºå‚æ•°
        åŸºäºé‡æ­£åŒ–ç¾¤åˆ†æï¼ˆç®—æ³•4.4.1ï¼‰
        """
        # æ£€æµ‹ä¸´ç•Œç‚¹
        if torch.abs(reflexive_load - self.critical_load) < 0.05:
            # åœ¨ä¸´ç•Œç‚¹é™„è¿‘ï¼Œè°ƒæ•´æŒ‡æ•°
            load_diff = reflexive_load - self.critical_load
            
            # æ›´æ–°æŒ‡æ•°ï¼ˆç®€åŒ–çš„é‡æ­£åŒ–ç¾¤æµï¼‰
            self.critical_exponents['nu'].data *= (1 + 0.01 * load_diff)
            self.critical_exponents['beta'].data *= (1 + 0.005 * entropy_gradient)
            self.critical_exponents['gamma'].data *= (1 + 0.002 * entropy_gradient)
            
            # æ›´æ–°Î©ä¼°è®¡
            self.omega_estimate.data = 0.5 * self.omega_estimate + 0.5 * torch.sigmoid(entropy_gradient)
        
        return self.critical_exponents

# ====================== 5. å®Œæ•´GTMLMæ¨¡å‹ ======================
class GTMLMComplete(nn.Module):
    """
    å®Œæ•´çš„GTMLMæ¨¡å‹
    æ•´åˆï¼šMamba + è®¤çŸ¥çº¤ç»´ä¸› + åèº«æ€§ç½‘ç»œ + é€»è¾‘ç†µ
    """
    def __init__(self, ntoken=1000, d_model=256, nlayers=8, 
                 fiber_dim=128, reflexive_layers=2):
        super().__init__()
        
        self.d_model = d_model
        self.ntoken = ntoken
        self.nlayers = nlayers
        
        # 1. åŸºç¡€ç¼–ç 
        self.encoder = nn.Embedding(ntoken, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        
        # 2. è®¤çŸ¥çº¤ç»´ä¸›
        self.fiber_bundle = CognitiveFiberBundle(
            base_dim=d_model, 
            fiber_dim=fiber_dim
        )
        
        # 3. åèº«æ€§è‡ªé€‚åº”ç½‘ç»œ
        self.reflexive_network = ReflexiveAdaptiveNetwork(
            node_dim=d_model,
            constraint_dim=64
        )
        
        # 4. å¢å¼ºMambaå±‚
        self.mamba_layers = nn.ModuleList()
        for i in range(nlayers):
            layer = EnhancedMambaBlock(
                d_model=d_model,
                d_state=16,
                d_conv=4,
                expand=2,
                fiber_bundle=self.fiber_bundle if i % 3 == 0 else None,  # æ¯3å±‚é›†æˆçº¤ç»´ä¸›
                reflexive_network=self.reflexive_network if i % 4 == 0 else None  # æ¯4å±‚é›†æˆåèº«æ€§
            )
            self.mamba_layers.append(layer)
        
        # 5. å…¬ç†è®°å¿†æ± 
        self.axiom_memory = nn.Parameter(
            torch.randn(MAX_AXIOMS, d_model) * 0.01
        )
        self.axiom_gate = nn.Parameter(torch.zeros(MAX_AXIOMS))
        self.axiom_attention = nn.MultiheadAttention(d_model, num_heads=4, batch_first=True)
        
        # 6. é€»è¾‘ç†µç›‘æ§
        self.entropy_monitor = LogicalEntropyMonitor()
        
        # 7. è¾“å‡ºå±‚ï¼ˆç”Ÿæ€ç†æ€§ï¼‰
        self.ecological_rationality = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Linear(d_model * 2, d_model),
            nn.GELU(),
            nn.Linear(d_model, N_CLASSES)
        )
        
        # 8. æ‹“æ‰‘æ­£åˆ™åŒ–ç»„ä»¶
        self.topological_loss_weight = nn.Parameter(torch.tensor(0.1))
        
        # åˆå§‹åŒ–
        self._initialize_parameters()
        
    def _initialize_parameters(self):
        """åˆå§‹åŒ–å‚æ•°ï¼Œç¡®ä¿ç¨³å®šè®­ç»ƒ"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, x, reflexive_load=0.5, return_components=False):
        B, L = x.shape
        
        # 1. ç¼–ç è¾“å…¥
        x_emb = self.encoder(x) * math.sqrt(self.d_model)
        x_emb = self.pos_encoding(x_emb)
        
        # 2. é€šè¿‡å¢å¼ºMambaå±‚
        layer_outputs = []
        logical_entropies = []
        
        for i, layer in enumerate(self.mamba_layers):
            layer_load = reflexive_load * (i + 1) / self.nlayers  # åˆ†å±‚åèº«æ€§è´Ÿè·
            x_emb = layer(x_emb, layer_load)
            layer_outputs.append(x_emb)
            
            # æ”¶é›†é€»è¾‘ç†µ
            if hasattr(layer, 'logical_entropy'):
                logical_entropies.append(layer.logical_entropy)
        
        # 3. å…¬ç†è®°å¿†æ± æ³¨æ„åŠ›ï¼ˆåèº«æ€§ï¼‰
        gate = torch.sigmoid(self.axiom_gate)
        weighted_memory = self.axiom_memory * gate.unsqueeze(-1)
        
        # è‡ªæ³¨æ„åŠ›ï¼šå½“å‰çŠ¶æ€æŸ¥è¯¢å…¬ç†è®°å¿†æ± 
        query = x_emb.mean(1, keepdim=True)  # [B, 1, D]
        key = value = weighted_memory.unsqueeze(0).expand(B, -1, -1)
        
        attn_output, attn_weights = self.axiom_attention(
            query, key, value
        )
        
        # èåˆ
        x_emb = x_emb + attn_output.expand(-1, L, -1)
        
        # 4. è®¡ç®—å‡ ä½•ä¸å˜é‡
        final_state = x_emb.mean(1)  # [B, D]
        
        # æ›²ç‡
        curvature = self.fiber_bundle.compute_curvature(final_state)
        
        # Holonomy
        hol = self.fiber_bundle.holonomy(final_state)
        
        # é™ˆ-è¥¿è’™æ–¯ä¸å˜é‡ï¼ˆé€»è¾‘ç†µçš„å‡ ä½•è¡¨ç¤ºï¼‰
        cs_invariant = self.fiber_bundle.compute_chern_simons()
        
        # 5. è®¡ç®—é€»è¾‘ç†µ
        system_complexity = sum(p.numel() for p in self.parameters())
        logical_entropy = self.entropy_monitor.compute_entropy(
            reflexive_load, system_complexity
        )
        
        # æ›´æ–°ä¸´ç•Œè¡Œä¸º
        if self.training:
            entropy_gradient = torch.autograd.grad(
                logical_entropy, 
                self.entropy_monitor.critical_load,
                retain_graph=True
            )[0]
            self.entropy_monitor.update_critical_behavior(
                reflexive_load, entropy_gradient
            )
        
        # 6. ç”Ÿæ€ç†æ€§è¾“å‡º
        logits = self.ecological_rationality(final_state)
        
        # 7. æ‹“æ‰‘æŸå¤±è®¡ç®—
        topological_loss = self._compute_topological_loss(weighted_memory)
        
        if return_components:
            components = {
                'logits': logits,
                'logical_entropy': logical_entropy,
                'curvature': curvature,
                'holonomy': hol,
                'chern_simons': cs_invariant,
                'topological_loss': topological_loss,
                'reflexive_load': reflexive_load,
                'axiom_gate': gate,
                'attention_weights': attn_weights,
                'layer_outputs': layer_outputs,
                'critical_exponents': dict(self.entropy_monitor.critical_exponents)
            }
            return components
        
        return logits, logical_entropy, topological_loss
    
    def _compute_topological_loss(self, memory):
        """
        è®¡ç®—æ‹“æ‰‘æŸå¤±ï¼ˆsoft k-means + æŒç»­æ€§åŒè°ƒï¼‰
        å¯¹åº”å®šç†1.2.2ï¼šçŠ¶æ€ç©ºé—´çš„ä¸›ç»“æ„
        """
        # 1. soft k-meansæŸå¤±
        dist_matrix = torch.cdist(memory, memory)
        temperature = 0.1
        soft_assign = F.softmax(-dist_matrix / temperature, dim=-1)
        
        # è®¡ç®—èšç±»ä¸­å¿ƒ
        centers = torch.matmul(soft_assign, memory) / soft_assign.sum(dim=1, keepdim=True)
        
        # é‡æ„æŸå¤±
        recon_loss = ((memory.unsqueeze(1) - centers.unsqueeze(0)).pow(2).sum(-1) * soft_assign).sum()
        
        # 2. æŒç»­æ€§åŒè°ƒå¯å‘å¼ï¼ˆç®€åŒ–ï¼‰
        # è®¡ç®—è·ç¦»çŸ©é˜µçš„Bettiæ•°å¯å‘å¼
        sorted_dist = torch.sort(dist_matrix.flatten())[0]
        persistence = sorted_dist[-10:] - sorted_dist[:10]  # æœ€å¤§æœ€å°è·ç¦»å·®
        
        # æ‹“æ‰‘å¤æ‚åº¦
        topo_complexity = persistence.mean()
        
        # æ€»æ‹“æ‰‘æŸå¤±
        total_loss = recon_loss + self.topological_loss_weight * topo_complexity
        
        return total_loss
    
    def compute_geometric_invariants(self, states):
        """
        è®¡ç®—å‡ ä½•ä¸å˜é‡ï¼ˆå®šç†6.1.2ï¼‰
        R âˆ C_network Â· (1 + Î»Â·R_reflex)
        """
        if states is None or len(states) == 0:
            return torch.tensor(0.0, device=device)
        
        # è®¡ç®—çº¤ç»´ä¸›ä¸å˜é‡
        fiber_states = self.fiber_bundle.fiber_proj(states)
        
        # è¿æ¥ä½œç”¨
        connected = torch.matmul(fiber_states, self.fiber_bundle.connection)
        
        # ç»“æ„å˜æ¢
        transformed = self.fiber_bundle.structure_net(connected)
        
        # Holonomy
        hol = torch.norm(transformed - connected, dim=-1).mean()
        
        # æ›²ç‡
        curv = self.fiber_bundle.compute_curvature(states)
        
        # ç½‘ç»œå¤æ‚åº¦
        network_complexity = sum(p.numel() for p in self.parameters())
        
        # åèº«æ€§è´¡çŒ®
        reflexive_contrib = torch.sigmoid(self.reflexive_network.reflexive_load)
        
        # æ€»å‡ ä½•ä¸å˜é‡
        geometric_invariant = (
            hol + curv + 
            0.01 * network_complexity * (1 + reflexive_contrib)
        )
        
        return geometric_invariant

# ====================== 6. ä½ç½®ç¼–ç  ======================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)

# ====================== 7. æ•°æ®é›†ï¼ˆå¿ å®äºåŸå§‹è®¾è®¡ï¼‰ ======================
class StructuredAlgebraDataset(Dataset):
    """
    å¿ å®äºåŸå§‹è®¾è®¡çš„ç»“æ„åŒ–ä»£æ•°æ•°æ®é›†
    åŸºäºAXIOM_CLUSTERSå’Œpreference_prob
    """
    def __init__(self, num_samples=10000, seq_len=50, 
                 vocab_size=1000, preference_prob=0.4):
        self.seq_len = seq_len
        self.vocab_size = vocab_size
        self.preference_prob = preference_prob
        
        # ç”Ÿæˆæ•°æ®
        self.data = []
        self.labels = []
        
        # æ™®é€štokenèŒƒå›´
        normal_tokens = list(range(min(AXIOM_VOCAB_START, vocab_size)))
        
        for _ in range(num_samples):
            # é€‰æ‹©ç±»åˆ«
            label = random.randint(0, N_CLASSES - 1)
            self.labels.append(label)
            
            # è·å–è¯¥ç±»å…¬ç†
            axiom_tokens = AXIOM_CLUSTERS[label]
            
            # ç”Ÿæˆåºåˆ—
            sequence = []
            for _ in range(seq_len):
                if random.random() < preference_prob:
                    # ä½¿ç”¨å…¬ç†token
                    token = random.choice(axiom_tokens)
                else:
                    # ä½¿ç”¨æ™®é€štokenï¼ˆç¡®ä¿åœ¨è¯æ±‡è¡¨å†…ï¼‰
                    token = random.choice(normal_tokens)
                sequence.append(token % vocab_size)  # ç¡®ä¿åœ¨è¯æ±‡è¡¨å†…
            
            self.data.append(sequence)
        
        # è½¬æ¢ä¸ºtensor
        self.data = torch.tensor(self.data, dtype=torch.long)
        self.labels = torch.tensor(self.labels, dtype=torch.long)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# ====================== 8. å…ƒå­¦ä¹ ä»»åŠ¡æ•°æ®é›† ======================
class MetaLearningTaskDataset(Dataset):
    """
    å…ƒå­¦ä¹ ä»»åŠ¡æ•°æ®é›†
    æ¯ä¸ªä»»åŠ¡æ˜¯ä¸€ä¸ªfew-shotå­¦ä¹ é—®é¢˜
    """
    def __init__(self, base_dataset, n_support=5, n_query=10, n_tasks=1000):
        self.base_dataset = base_dataset
        self.n_support = n_support
        self.n_query = n_query
        self.n_tasks = n_tasks
        
        # æŒ‰ç±»åˆ«ç´¢å¼•
        self.class_indices = [[] for _ in range(N_CLASSES)]
        for idx in range(len(base_dataset)):
            _, label = base_dataset[idx]
            self.class_indices[label.item()].append(idx)
        
        # ç”Ÿæˆä»»åŠ¡
        self.tasks = []
        for _ in range(n_tasks):
            # éšæœºé€‰æ‹©ç±»åˆ«ï¼ˆ5-wayï¼‰
            task_class = random.randint(0, N_CLASSES - 1)
            indices = self.class_indices[task_class]
            
            if len(indices) >= n_support + n_query:
                selected = random.sample(indices, n_support + n_query)
                
                task = {
                    'class': task_class,
                    'support_idx': selected[:n_support],
                    'query_idx': selected[n_support:]
                }
                self.tasks.append(task)
    
    def __len__(self):
        return len(self.tasks)
    
    def __getitem__(self, idx):
        task = self.tasks[idx]
        
        # æ”¯æŒé›†
        support_data = [self.base_dataset[i] for i in task['support_idx']]
        support_x = torch.stack([x for x, _ in support_data])
        support_y = torch.stack([torch.tensor(task['class']) for _ in support_data])
        
        # æŸ¥è¯¢é›†
        query_data = [self.base_dataset[i] for i in task['query_idx']]
        query_x = torch.stack([x for x, _ in query_data])
        query_y = torch.stack([torch.tensor(task['class']) for _ in query_data])
        
        return {
            'support': (support_x, support_y),
            'query': (query_x, query_y),
            'class': task['class']
        }

# ====================== 9. è®¤çŸ¥çƒ­åŠ›å­¦è®­ç»ƒå™¨ ======================
class CognitiveThermodynamicsTrainer:
    """
    è®¤çŸ¥çƒ­åŠ›å­¦è®­ç»ƒå™¨
    åŸºäºé€»è¾‘ç†µå¢å®šå¾‹å’Œåèº«æ€§å¥‡ç‚¹å®šç†
    """
    def __init__(self, model, meta_lr=1e-4, inner_lr=0.01):
        self.model = model
        self.meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)
        
        # é€»è¾‘æ¸©åº¦è°ƒåº¦å™¨
        self.entropy_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.meta_optimizer, T_max=100, eta_min=1e-6
        )
        
        # åèº«æ€§è´Ÿè·è°ƒåº¦ï¼ˆå®šç†4.1ï¼‰
        self.reflexive_load = 0.1  # åˆå§‹ä½è´Ÿè·
        self.load_increment = 0.01
        
        # è®­ç»ƒè®°å½•
        self.entropy_history = []
        self.curvature_history = []
        self.accuracy_history = []
    
    def train_step(self, support, query, inner_steps=5):
        """
        è®¤çŸ¥çƒ­åŠ›å­¦è®­ç»ƒæ­¥éª¤
        å®šç†4.2.5ï¼šLyapunovç¨³å®šæ€§åˆ†æ
        """
        support_x, support_y = support
        query_x, query_y = query
        
        # å†…å¾ªç¯ï¼šä»»åŠ¡é€‚åº”ï¼ˆå¿«å°ºåº¦ï¼‰
        fast_model = self._clone_model()
        inner_optimizer = optim.SGD(fast_model.parameters(), lr=inner_lr)
        
        inner_losses = []
        for step in range(inner_steps):
            # å¢åŠ åèº«æ€§è´Ÿè·ï¼ˆæ¨¡æ‹Ÿè®¤çŸ¥å‹åŠ›ï¼‰
            current_load = self.reflexive_load + step * 0.01
            
            # å‰å‘ä¼ æ’­
            logits, entropy, topo_loss = fast_model(support_x, current_load)
            
            # è®¡ç®—æŸå¤±ï¼ˆè®¤çŸ¥è‡ªç”±èƒ½ï¼‰
            task_loss = F.cross_entropy(logits, support_y)
            
            # æ€»æŸå¤± = ä»»åŠ¡æŸå¤± + é€»è¾‘ç†µ + æ‹“æ‰‘æŸå¤±
            total_loss = task_loss + 0.1 * entropy + 0.01 * topo_loss
            
            # åå‘ä¼ æ’­
            inner_optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢è®¤çŸ¥å¥‡ç‚¹ï¼‰
            torch.nn.utils.clip_grad_norm_(fast_model.parameters(), 1.0)
            
            inner_optimizer.step()
            inner_losses.append(total_loss.item())
        
        # å¤–å¾ªç¯ï¼šå…ƒæ›´æ–°ï¼ˆæ…¢å°ºåº¦ï¼‰
        self.meta_optimizer.zero_grad()
        
        # åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°
        with torch.no_grad():
            q_logits, q_entropy, q_topo_loss = fast_model(query_x, self.reflexive_load)
            q_loss = F.cross_entropy(q_logits, query_y)
            
            # è®¡ç®—å‡ ä½•ä¸å˜é‡
            final_state = fast_model.mamba_layers[-1](query_x.mean(1, keepdim=True))
            geometric_loss = fast_model.compute_geometric_invariants(final_state)
        
        # å…ƒæŸå¤±ï¼ˆè®¤çŸ¥çƒ­åŠ›å­¦ç¬¬ä¸€å®šå¾‹ï¼‰
        meta_loss = q_loss + 0.05 * q_entropy + 0.01 * q_topo_loss + 0.001 * geometric_loss
        
        # åå‘ä¼ æ’­
        meta_loss.backward()
        
        # åº”ç”¨Reptileæ›´æ–°ï¼ˆå®šç†3.2.2ï¼‰
        self._reptile_update(fast_model)
        
        # æ›´æ–°å‚æ•°
        self.meta_optimizer.step()
        
        # æ›´æ–°åèº«æ€§è´Ÿè·ï¼ˆæ¨¡æ‹Ÿè®¤çŸ¥æ¼”åŒ–ï¼‰
        self.reflexive_load = min(0.9, self.reflexive_load + self.load_increment)
        
        # è®°å½•
        self.entropy_history.append(q_entropy.item())
        self.curvature_history.append(geometric_loss.item())
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = (q_logits.argmax(-1) == query_y).float().mean().item()
        self.accuracy_history.append(accuracy)
        
        return meta_loss.item(), accuracy, q_entropy.item()
    
    def _clone_model(self):
        """å…‹éš†æ¨¡å‹ç”¨äºå†…å¾ªç¯"""
        cloned = type(self.model)(
            ntoken=self.model.ntoken,
            d_model=self.model.d_model,
            nlayers=self.model.nlayers
        ).to(device)
        cloned.load_state_dict(self.model.state_dict())
        return cloned
    
    def _reptile_update(self, fast_model):
        """
        Reptileæ›´æ–°ï¼šä½¿å‚æ•°å‘é€‚åº”åå‚æ•°é è¿‘
        å®šç†2.2.2ï¼šæ‹“æ‰‘æ¼”åŒ–çš„ç¨³å®šæ€§
        """
        reptile_lr = 0.1
        
        for param, fast_param in zip(self.model.parameters(), fast_model.parameters()):
            if param.grad is None:
                param.grad = torch.zeros_like(param)
            
            # Reptileæ¢¯åº¦ï¼šfast_param - param
            reptile_grad = (fast_param.detach() - param.detach()) * reptile_lr
            param.grad.add_(reptile_grad)

# ====================== 10. ä¸»è®­ç»ƒå¾ªç¯ ======================
def train_gtmlm_complete():
    """å®Œæ•´GTMLMè®­ç»ƒ"""
    print("=" * 60)
    print("GTMLM Mamba Ultimate - å®Œæ•´è®­ç»ƒ")
    print("åŸºäºï¼šåèº«æ€§è‡ªé€‚åº”ç½‘ç»œ + é€»è¾‘çš„ç†µ + å½¢å¼è®¤è¯†è®º")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # 1. åˆ›å»ºæ•°æ®é›†
    print("åˆ›å»ºæ•°æ®é›†...")
    base_dataset = StructuredAlgebraDataset(
        num_samples=10000,
        seq_len=50,
        vocab_size=1000,
        preference_prob=0.4
    )
    
    meta_dataset = MetaLearningTaskDataset(
        base_dataset=base_dataset,
        n_support=5,
        n_query=10,
        n_tasks=500
    )
    
    meta_loader = DataLoader(meta_dataset, batch_size=1, shuffle=True)
    
    # 2. åˆ›å»ºå®Œæ•´GTMLMæ¨¡å‹
    print("åˆå§‹åŒ–å®Œæ•´GTMLMæ¨¡å‹...")
    model = GTMLMComplete(
        ntoken=1000,
        d_model=256,
        nlayers=8,
        fiber_dim=128,
        reflexive_layers=2
    ).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # 3. åˆ›å»ºè®¤çŸ¥çƒ­åŠ›å­¦è®­ç»ƒå™¨
    trainer = CognitiveThermodynamicsTrainer(model)
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\nå¼€å§‹è®¤çŸ¥çƒ­åŠ›å­¦è®­ç»ƒ...")
    print("=" * 60)
    
    num_epochs = 20
    best_accuracy = 0
    
    for epoch in range(num_epochs):
        model.train()
        epoch_losses = []
        epoch_accuracies = []
        epoch_entropies = []
        
        for i, task_batch in enumerate(meta_loader):
            if i >= 100:  # æ¯ä¸ªepoch 100ä¸ªä»»åŠ¡
                break
            
            support = task_batch['support']
            query = task_batch['query']
            
            # è®­ç»ƒæ­¥éª¤
            loss, acc, entropy = trainer.train_step(support, query)
            
            epoch_losses.append(loss)
            epoch_accuracies.append(acc)
            epoch_entropies.append(entropy)
            
            if i % 10 == 0:
                print(f"Epoch {epoch+1}, Task {i}: "
                      f"Loss={loss:.4f}, Acc={acc:.4f}, Entropy={entropy:.4f}")
        
        # è®¡ç®—epochç»Ÿè®¡
        avg_loss = np.mean(epoch_losses)
        avg_acc = np.mean(epoch_accuracies)
        avg_entropy = np.mean(epoch_entropies)
        
        print(f"\nEpoch {epoch+1} ç»Ÿè®¡:")
        print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")
        print(f"  å¹³å‡é€»è¾‘ç†µ: {avg_entropy:.4f}")
        print(f"  å½“å‰åèº«æ€§è´Ÿè·: {trainer.reflexive_load:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_acc > best_accuracy:
            best_accuracy = avg_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.meta_optimizer.state_dict(),
                'accuracy': avg_acc,
                'entropy': avg_entropy,
            }, "best_gtmlm_model.pth")
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {avg_acc:.4f}")
        
        print("-" * 40)
    
    # 5. æœ€ç»ˆè¯„ä¼°
    print("\næœ€ç»ˆè¯„ä¼°...")
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•é›†
    test_dataset = StructuredAlgebraDataset(
        num_samples=1000,
        seq_len=50,
        vocab_size=1000,
        preference_prob=0.3
    )
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    total_correct = 0
    total_samples = 0
    test_entropies = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            # ä½¿ç”¨æœ€ç»ˆåèº«æ€§è´Ÿè·
            logits, entropy, _ = model(batch_x, trainer.reflexive_load)
            
            predictions = logits.argmax(-1)
            correct = (predictions == batch_y).sum().item()
            
            total_correct += correct
            total_samples += batch_y.size(0)
            test_entropies.append(entropy.item())
    
    final_accuracy = total_correct / total_samples
    avg_test_entropy = np.mean(test_entropies)
    
    print("=" * 60)
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"æœ€ç»ˆæµ‹è¯•é€»è¾‘ç†µ: {avg_test_entropy:.4f}")
    print(f"æœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {best_accuracy:.4f}")
    
    # 6. è¾“å‡ºç†è®ºéªŒè¯æŒ‡æ ‡
    print("\nç†è®ºéªŒè¯æŒ‡æ ‡:")
    print("-" * 40)
    
    # è·å–æ¨¡å‹çš„å‡ ä½•ä¸å˜é‡
    test_sample = torch.randint(0, 1000, (1, 50)).to(device)
    components = model(test_sample, return_components=True)
    
    print(f"æ›²ç‡: {components['curvature'].item():.6f}")
    print(f"Holonomy: {components['holonomy'].item():.6f}")
    print(f"é™ˆ-è¥¿è’™æ–¯ä¸å˜é‡: {components['chern_simons'].item():.6f}")
    print(f"ä¸´ç•ŒæŒ‡æ•°:")
    for name, param in components['critical_exponents'].items():
        print(f"  {name}: {param.item():.3f}")
    
    print("=" * 60)
    
    return model, final_accuracy

# ====================== 11. è¿è¡Œä¸»ç¨‹åº ======================
if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´è®­ç»ƒ
    try:
        model, accuracy = train_gtmlm_complete()
        
        if accuracy > 0.7:
            print("\nğŸ‰ GTMLMè®­ç»ƒæˆåŠŸï¼æ¨¡å‹å­¦ä¼šäº†ç»“æ„åŒ–ä»£æ•°æ¨ç†ã€‚")
            print("è¿™è¡¨æ˜ï¼š")
            print("1. è®¤çŸ¥çº¤ç»´ä¸›ç†è®ºåœ¨å®è·µä¸­æœ‰æ•ˆ")
            print("2. åèº«æ€§è‡ªé€‚åº”ç½‘ç»œå¯ä»¥å»ºæ¨¡è®¤çŸ¥æ¼”åŒ–")
            print("3. é€»è¾‘ç†µå¢å®šå¾‹åœ¨è®­ç»ƒä¸­å¯è§‚å¯Ÿåˆ°")
            print("4. å‡ ä½•æ‹“æ‰‘ç»“æ„å¢å¼ºäº†æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›")
        else:
            print("\nâš ï¸  è®­ç»ƒå®Œæˆï¼Œä½†å‡†ç¡®ç‡æœ‰å¾…æé«˜ã€‚")
            print("å¯èƒ½çš„åŸå› ï¼š")
            print("1. æ¨¡å‹å¤æ‚åº¦å¯èƒ½ä¸è¶³")
            print("2. è®­ç»ƒæ—¶é—´ä¸å¤Ÿ")
            print("3. éœ€è¦è°ƒæ•´è¶…å‚æ•°")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
